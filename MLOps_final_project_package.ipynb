{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLOps_final_project_package.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fqGNZKzoaV4f",
        "5a_Jm_G0ehc6",
        "z2hcI8EqeAux",
        "UbLXTf6nkoOS",
        "UMYl05c9t0SD",
        "HqRByuXibQn2",
        "0zVS0ZIxd-8f",
        "FQMtZ3Zth1p2",
        "z31oVySbh3Wt",
        "2qnoofGHaqsS",
        "_ES-jEEbC7jg",
        "y7tOTjJUUAkz",
        "d-UeR7WuFjLS",
        "8rgExFRRFtZ1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqGNZKzoaV4f"
      },
      "source": [
        "#### Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08bAMqBVZlaI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a82c9ec0-5837-4c81-cc28-345d08917275"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a_Jm_G0ehc6"
      },
      "source": [
        "#### Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqBS2pZGjx1r",
        "outputId": "0a494206-5149-4e79-de36-55432356425e"
      },
      "source": [
        "!pip install graphviz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "20QJs3MlCQuh",
        "outputId": "348be42a-7e1b-4c83-d7ad-57ecd3da58c1"
      },
      "source": [
        "pip install ydata-synthetic"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ydata-synthetic\n",
            "  Downloading ydata_synthetic-0.5.0-py2.py3-none-any.whl (43 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▌                        | 10 kB 17.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 30 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn==0.11.* in /usr/local/lib/python3.7/dist-packages (from ydata-synthetic) (0.11.2)\n",
            "Collecting pmlb==1.0.*\n",
            "  Downloading pmlb-1.0.1.post3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: numpy==1.19.* in /usr/local/lib/python3.7/dist-packages (from ydata-synthetic) (1.19.5)\n",
            "Collecting tensorflow==2.4.*\n",
            "  Downloading tensorflow-2.4.4-cp37-cp37m-manylinux2010_x86_64.whl (394.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 394.5 MB 36 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0 in /usr/local/lib/python3.7/dist-packages (from ydata-synthetic) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn==1.0.* in /usr/local/lib/python3.7/dist-packages (from ydata-synthetic) (1.0.1)\n",
            "Requirement already satisfied: easydict==1.9 in /usr/local/lib/python3.7/dist-packages (from ydata-synthetic) (1.9)\n",
            "Collecting matplotlib==3.3.2\n",
            "  Downloading matplotlib-3.3.2-cp37-cp37m-manylinux1_x86_64.whl (11.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.6 MB 10.6 MB/s \n",
            "\u001b[?25hCollecting pandas==1.2.*\n",
            "  Downloading pandas-1.2.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.2->ydata-synthetic) (1.3.2)\n",
            "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.2->ydata-synthetic) (2021.10.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.2->ydata-synthetic) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.2->ydata-synthetic) (3.0.6)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.2->ydata-synthetic) (7.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.2->ydata-synthetic) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.*->ydata-synthetic) (2018.9)\n",
            "Collecting requests>=2.24.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 843 kB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 35.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.*->ydata-synthetic) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.*->ydata-synthetic) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.*->ydata-synthetic) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.*->ydata-synthetic) (3.17.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.*->ydata-synthetic) (0.37.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting h5py~=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 24.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.*->ydata-synthetic) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.*->ydata-synthetic) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.*->ydata-synthetic) (1.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.*->ydata-synthetic) (2.7.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.*->ydata-synthetic) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.*->ydata-synthetic) (3.3.0)\n",
            "Collecting grpcio~=1.32.0\n",
            "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 14.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.5.0,>=2.4.0\n",
            "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 40.6 MB/s \n",
            "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.*->ydata-synthetic) (0.12.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.*->ydata-synthetic) (1.6.3)\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pmlb==1.0.*->ydata-synthetic) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pmlb==1.0.*->ydata-synthetic) (2.0.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pmlb==1.0.*->ydata-synthetic) (2.10)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.*->ydata-synthetic) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.*->ydata-synthetic) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.*->ydata-synthetic) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.*->ydata-synthetic) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.*->ydata-synthetic) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.*->ydata-synthetic) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.*->ydata-synthetic) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.*->ydata-synthetic) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.*->ydata-synthetic) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.*->ydata-synthetic) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.*->ydata-synthetic) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.*->ydata-synthetic) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.*->ydata-synthetic) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.*->ydata-synthetic) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.*->ydata-synthetic) (3.1.1)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68720 sha256=0a625aabd2d03dbbf4738322d5b46883a71784864c5309c64dc2135068b5891d\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built wrapt\n",
            "Installing collected packages: typing-extensions, requests, grpcio, wrapt, tensorflow-estimator, pyyaml, pandas, matplotlib, h5py, gast, flatbuffers, tensorflow, pmlb, ydata-synthetic\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.42.0\n",
            "    Uninstalling grpcio-1.42.0:\n",
            "      Successfully uninstalled grpcio-1.42.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.13.3\n",
            "    Uninstalling wrapt-1.13.3:\n",
            "      Successfully uninstalled wrapt-1.13.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.2.5 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed flatbuffers-1.12 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 matplotlib-3.3.2 pandas-1.2.5 pmlb-1.0.1.post3 pyyaml-6.0 requests-2.26.0 tensorflow-2.4.4 tensorflow-estimator-2.4.0 typing-extensions-3.7.4.3 wrapt-1.12.1 ydata-synthetic-0.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQJtxae5EZSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0acf78b-2777-4a60-8eb9-8620da5e9f42"
      },
      "source": [
        "pip install pymc3"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymc3 in /usr/local/lib/python3.7/dist-packages (3.11.4)\n",
            "Requirement already satisfied: arviz>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from pymc3) (0.11.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from pymc3) (0.3.4)\n",
            "Requirement already satisfied: cachetools>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from pymc3) (4.2.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pymc3) (1.4.1)\n",
            "Requirement already satisfied: semver>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from pymc3) (2.13.0)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from pymc3) (0.5.2)\n",
            "Requirement already satisfied: theano-pymc==1.1.2 in /usr/local/lib/python3.7/dist-packages (from pymc3) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from pymc3) (1.19.5)\n",
            "Requirement already satisfied: fastprogress>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pymc3) (1.0.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from pymc3) (1.2.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from pymc3) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from theano-pymc==1.1.2->pymc3) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from arviz>=0.11.0->pymc3) (21.3)\n",
            "Requirement already satisfied: xarray>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from arviz>=0.11.0->pymc3) (0.18.2)\n",
            "Requirement already satisfied: netcdf4 in /usr/local/lib/python3.7/dist-packages (from arviz>=0.11.0->pymc3) (1.5.8)\n",
            "Requirement already satisfied: setuptools>=38.4 in /usr/local/lib/python3.7/dist-packages (from arviz>=0.11.0->pymc3) (57.4.0)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from arviz>=0.11.0->pymc3) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.11.0->pymc3) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.11.0->pymc3) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.11.0->pymc3) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.11.0->pymc3) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.11.0->pymc3) (1.3.2)\n",
            "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.11.0->pymc3) (2021.10.8)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->pymc3) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->pymc3) (1.15.0)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.7/dist-packages (from netcdf4->arviz>=0.11.0->pymc3) (1.5.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjGyJ6u_OBa_",
        "outputId": "ca4f38dd-db67-436b-e51f-a8aae147fb19"
      },
      "source": [
        "!pip install pyyaml==5.4.1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyyaml==5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 8.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "Successfully installed pyyaml-5.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2hcI8EqeAux"
      },
      "source": [
        "#### Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQBaTtJjaVbX"
      },
      "source": [
        "#Importing required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pymc3\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import svm\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn import tree\n",
        "from ydata_synthetic.synthesizers.regular import WGAN_GP\n",
        "from ydata_synthetic.synthesizers import ModelParameters, TrainParameters\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import graphviz\n",
        "from scipy.stats import norm\n",
        "%matplotlib inline"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLXTf6nkoOS"
      },
      "source": [
        "#### Data Processing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oFI_0PkkyP5"
      },
      "source": [
        "# initalize data-set object \n",
        "def initalize_data_set(target_column, predicted_column, categorical_threshold = 0.001, csv_file_path = None , df = None):\n",
        "  if csv_file_path is None and df is None:\n",
        "    raise Exception(\"expected csv file path or data frame object\")\n",
        "  if df is None:\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "  else:\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "      raise Exception(\"data frame object must be of type 'pandas.core.frame.DataFrame'\")\n",
        "      \n",
        "  if predicted_column == \"\":\n",
        "    X = df.drop([target_column], axis=1)\n",
        "    Y = df[target_column]\n",
        "  else:\n",
        "    X = df.drop([target_column, predicted_column], axis=1)\n",
        "    Y = (df[predicted_column] == df[target_column])\n",
        "\n",
        "  categorical_features = {}\n",
        "  for feature in X.columns:\n",
        "      categorical_features[feature] = 1.*X[feature].nunique()/X[feature].count() < categorical_threshold or X[feature].dtype == \"object\"\n",
        "\n",
        "  X_encoded = pd.get_dummies(X, columns= [key for (key, value) in categorical_features.items() if value ])\n",
        "\n",
        "  return X_encoded, Y, categorical_features"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMYl05c9t0SD"
      },
      "source": [
        "####HDR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Juve3aLOF4kF"
      },
      "source": [
        "def hdr(df_, Y, column, eps = 0.05, threshold = 0.01):\n",
        "  df = df_.copy()\n",
        "  df['is_correct_pred'] = Y\n",
        "  \n",
        "  range_diff_history = []\n",
        "  start, end = pymc3.stats.hdi(df[column].values, hdi_prob=0.5)\n",
        "\n",
        "  data_interval = df[df[column].between(start, end)]\n",
        "  error_rate = (data_interval['is_correct_pred'] == False).sum() / df.shape[0]\n",
        "\n",
        "  while (df[df[column].between(start, end)].shape[0] / df.shape[0] > 0.1):\n",
        "\n",
        "    prev_start = start\n",
        "    prev_end = end\n",
        "\n",
        "    start = start * (1 + eps)\n",
        "    end = end * (1 - eps)\n",
        "\n",
        "    data_interval = df[df[column].between(start, end)]\n",
        "    new_error_rate = (data_interval['is_correct_pred'] == False).sum() / df.shape[0]\n",
        "\n",
        "    if (error_rate - new_error_rate > threshold):\n",
        "      range_diff = {\n",
        "          'error_rate' : error_rate - new_error_rate,\n",
        "          'range_start' : (prev_start, start),\n",
        "          'range_end': (end, prev_end)\n",
        "      }\n",
        "      range_diff_history.append(range_diff)\n",
        "    error_rate = new_error_rate\n",
        "  N = int(0.25 * len(range_diff_history))\n",
        "  n_largest_diffs = sorted(range_diff_history, key=lambda t: t['error_rate'], reverse=True)[:N]\n",
        "\n",
        "  return n_largest_diffs\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LU43q2iRoRj"
      },
      "source": [
        "def hdr_ranges_to_slice(df, column, ranges):\n",
        "  df_slices = []\n",
        "  for range in ranges:\n",
        "    df_range = df[df[column].between(range['range_start'][0], range['range_start'][1]) | df[column].between(range['range_end'][0], range['range_end'][1])]\n",
        "    df_slices.append(df_range)\n",
        "  return pd.concat(df_slices)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqRByuXibQn2"
      },
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_CId7eq-rlP"
      },
      "source": [
        "def get_lineage(tree, feature_names):\n",
        "     left = tree.tree_.children_left\n",
        "     right     = tree.tree_.children_right\n",
        "     threshold = tree.tree_.threshold\n",
        "     features  = [feature_names[i] if i != -2 else -5 for i in tree.tree_.feature]\n",
        "\n",
        "     # get ids of child nodes\n",
        "     idx = np.argwhere(left == -1)[:,0]     \n",
        "\n",
        "     def recurse(left, right, child, lineage=None):          \n",
        "          if lineage is None:\n",
        "               lineage = [child]\n",
        "          if child in left:\n",
        "               parent = np.where(left == child)[0].item()\n",
        "               split = 'l'\n",
        "          else:\n",
        "               parent = np.where(right == child)[0].item()\n",
        "               split = 'r'\n",
        "\n",
        "          lineage.append((parent, split, threshold[parent], features[parent]))\n",
        "\n",
        "          if parent == 0:\n",
        "               lineage.reverse()\n",
        "               return lineage\n",
        "          else:\n",
        "               return recurse(left, right, parent, lineage)\n",
        "    \n",
        "     childs = {}\n",
        "\n",
        "     for child in idx:\n",
        "          child_rules = []\n",
        "          for node in recurse(left, right, child):\n",
        "               child_rules.append(node)\n",
        "          childs[child] = child_rules\n",
        "     return childs\n",
        "    \n",
        "               "
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdlQFs3PZw_E"
      },
      "source": [
        "def get_leaves_props(leaves, tree_clf):\n",
        "  leaves_range_dict = {}\n",
        "  for key,value in leaves.items():\n",
        "    range_min_max_dict = {}\n",
        "    for range_tuple in value:\n",
        "      if type(range_tuple) == tuple:\n",
        "        if range_tuple[3] not in range_min_max_dict:\n",
        "          range_min_max_dict[range_tuple[3]] = {}\n",
        "        if (range_tuple[1] == 'r'):\n",
        "          if 'min' not in range_min_max_dict[range_tuple[3]]:\n",
        "            range_min_max_dict[range_tuple[3]]['min'] = range_tuple[2]\n",
        "          else:\n",
        "            if range_tuple[2] < range_min_max_dict[range_tuple[3]]['min']:\n",
        "              range_min_max_dict[range_tuple[3]]['min'] = range_tuple[2]\n",
        "        elif (range_tuple[1] == 'l'):\n",
        "          if 'max' not in range_min_max_dict[range_tuple[3]]:\n",
        "            range_min_max_dict[range_tuple[3]]['max'] = range_tuple[2]\n",
        "          else:\n",
        "            if range_tuple[2] > range_min_max_dict[range_tuple[3]]['max']:\n",
        "              range_min_max_dict[range_tuple[3]]['max'] = range_tuple[2]\n",
        "    leaves_range_dict[key] = {}\n",
        "    leaves_range_dict[key]['range'] = range_min_max_dict\n",
        "    leaves_range_dict[key]['relative_error_rate'] = tree_clf.tree_.value[key][0][0] / tree_clf.tree_.value[0][0][0]\n",
        "    leaves_range_dict[key]['error_rate'] = tree_clf.tree_.value[key][0][0] / (tree_clf.tree_.value[key][0][0] + tree_clf.tree_.value[key][0][1])\n",
        "  return leaves_range_dict\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa7qY15Lp7u7"
      },
      "source": [
        "def get_stat_important_leaves(leaves, tree_clf):\n",
        "  return dict(filter(lambda elem: elem[1]['relative_error_rate'] * tree_clf.tree_.value[0][0][1] > max(2, 0.05 * tree_clf.tree_.value[0][0][1]), leaves.items()))"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX7Wp9d6scuv"
      },
      "source": [
        "def get_decision_tree_slices(X, Y, cols):\n",
        "  tree_clf = tree.DecisionTreeClassifier()\n",
        "  tree_clf = tree_clf.fit(X[cols], Y)\n",
        "  dot_data = tree.export_graphviz(tree_clf, out_file=None, \n",
        "                      filled=True, rounded=True,  \n",
        "                      special_characters=True)  \n",
        "  graph = graphviz.Source(dot_data)  \n",
        "  leaves = get_lineage(tree_clf, X[cols].columns)\n",
        "  slices = get_stat_important_leaves(get_leaves_props(leaves, tree_clf), tree_clf)\n",
        "  slices_list = []\n",
        "  for key in slices.keys():\n",
        "    slices_list.append(slices[key])\n",
        "  return slices_list, graph"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J383DZzQt335"
      },
      "source": [
        "def get_slices_by_range(df, range):\n",
        "  vec = np.ones(df.shape[0], dtype=bool)\n",
        "  for key in range.keys():\n",
        "    vec = vec & df[key].between(range[key].get('min', float('-inf')), range[key].get('max', float('inf')))\n",
        "  return df[vec]\n",
        "\n",
        "def tree_indexes_by_slices(df, slices):\n",
        "  if len(slices) == 0:\n",
        "    return []\n",
        "  df_slices = []\n",
        "  for slice_ in slices:\n",
        "    for inner_slice in slice_:\n",
        "      df_range = get_slices_by_range(df, inner_slice['range'])\n",
        "      df_slices.append(df_range)\n",
        "  return pd.concat(df_slices)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zVS0ZIxd-8f"
      },
      "source": [
        "#### Apply Heuristics Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCmQIq8T8hAE"
      },
      "source": [
        "from heapq import nlargest\n",
        "\n",
        "def apply_heuristics(X, Y, features, options = {}):\n",
        "  high_rate_columns = [column for column in df.columns if df[column].value_counts().max() / df.shape[0] > 0.7 ]\n",
        "\n",
        "  categorical_features = [key for (key, value) in features.items() if (value and (key not in high_rate_columns))]\n",
        "  continious_features = [key for (key, value) in features.items() if ((value == False) and (key not in high_rate_columns))]\n",
        "  #categorical_features\n",
        "  categorical_features_error_rates_single = {}\n",
        "  categorical_features_slices_single = {}\n",
        "  for feature in categorical_features:\n",
        "    cols = [c for c in X.columns if f'{feature}_' in c]\n",
        "    slices, graph = get_decision_tree_slices(X, Y, cols)\n",
        "    if len(slices) > 0:\n",
        "      categorical_features_error_rates_single[feature] = np.mean([slice_dict['error_rate'] for slice_dict in slices])\n",
        "      categorical_features_slices_single[feature] = slices\n",
        "    \n",
        "  #continious_features\n",
        "  continious_feature_error_rates_single = {}\n",
        "  continious_feature_slices_single = {}\n",
        "  for feature in continious_features:\n",
        "    slices = hdr(X, Y, feature, options.get('eps', 0.05), options.get('hdr_threshold', 0.001)) \n",
        "    if len(slices) > 0:\n",
        "      continious_feature_error_rates_single[feature] = np.mean([slice_dict['error_rate'] for slice_dict in slices])\n",
        "      continious_feature_slices_single[feature] = slices\n",
        "\n",
        "  #combined top quarter\n",
        "  combined_features_slices_single = categorical_features_slices_single.copy()\n",
        "  combined_features_slices_single.update(continious_feature_slices_single)\n",
        "  combined_features_error_rates_single = categorical_features_error_rates_single.copy()\n",
        "  combined_features_error_rates_single.update(continious_feature_error_rates_single)\n",
        "  N = int(0.25 * len(combined_features_error_rates_single.keys()))\n",
        "  largest_feature_error_rates_single = nlargest(N, combined_features_error_rates_single, key = combined_features_error_rates_single.get)\n",
        "  \n",
        "  #feature pairs\n",
        "  filtered_features = [x for x in features if (x not in set(largest_feature_error_rates_single) and x not in high_rate_columns)]\n",
        "  feature_error_rates_pairs = {}\n",
        "  feature_slices_pairs = {}\n",
        "  for feature_largest in largest_feature_error_rates_single:\n",
        "    for feature in filtered_features:\n",
        "      cols = [c for c in X.columns if f'{feature_largest}_' in c or f'{feature}_' in c]\n",
        "      slices, graph = get_decision_tree_slices(X, Y, cols)\n",
        "\n",
        "      if len([slice_dict['error_rate'] for slice_dict in slices])!=0:\n",
        "        feature_error_rates_pairs[f'{feature_largest}_{feature}'] = np.mean([slice_dict['error_rate'] for slice_dict in slices])\n",
        "        feature_slices_pairs[f'{feature_largest}_{feature}'] = slices\n",
        "\n",
        "  N = int(0.25 * len(feature_error_rates_pairs.keys()))\n",
        "  largest_feature_error_rates_pairs = nlargest(N, feature_error_rates_pairs, key = feature_error_rates_pairs.get)\n",
        "\n",
        "  combined_features_slices_single.update(feature_slices_pairs)\n",
        "  combined_features_error_rates_single.update(feature_error_rates_pairs)\n",
        "  N = int(0.25 * len(combined_features_slices_single.keys()))\n",
        "  largest_feature_error_rates_combined = nlargest(N, combined_features_error_rates_single, key = combined_features_error_rates_single.get)\n",
        "\n",
        "  top_features_slices = {field:max(slice_list, key=lambda x:x['error_rate']) for (field,slice_list) in combined_features_slices_single.items()}\n",
        "\n",
        "  return top_features_slices, largest_feature_error_rates_combined"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVDds2HBcsFq"
      },
      "source": [
        "### Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQMtZ3Zth1p2"
      },
      "source": [
        "####reweighting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8_nXLA1fA3x"
      },
      "source": [
        "def get_all_indexes_from_all_slices(df, slices):\n",
        "  categorical_slices = []\n",
        "  continious_slices = []\n",
        "  for key in slices.keys():\n",
        "    # for categorical\n",
        "    if 'range' in slices[key][0]:\n",
        "      categorical_slices.append(slices[key])\n",
        "    else:\n",
        "      continious_slices.append(hdr_ranges_to_slice(df, key, slices[key]))\n",
        "\n",
        "  df_categorical = tree_indexes_by_slices(df, categorical_slices)\n",
        "  df_continious = None\n",
        "  if len(continious_slices) > 0:\n",
        "    df_continious = pd.concat(continious_slices)\n",
        "  if len(continious_slices) > 0 and len(df_categorical) > 0:\n",
        "    return pd.concat([df_categorical, df_continious]).index.unique()\n",
        "  elif len(df_categorical) == 0:\n",
        "    return df_continious.index.unique()\n",
        "  else:\n",
        "    return df_categorical.index.unique()"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6_L84sVvO1J"
      },
      "source": [
        "def reweighting(train_df, target_column, indexes, weight = 5):\n",
        "  sample_weights = np.ones(train_df.shape[0]) \n",
        "  sample_weights[indexes] = weight\n",
        "\n",
        "  clf = XGBClassifier()\n",
        "  clf.fit(train_df.drop(columns = [target_column], axis = 0), train_df[target_column], sample_weight = sample_weights)\n",
        "\n",
        "  return clf"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z31oVySbh3Wt"
      },
      "source": [
        "####Synthesized Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvZGWxtWa_86"
      },
      "source": [
        "def synthesized_data(train_samples, generator_sample_size):\n",
        "\n",
        "  #config\n",
        "  noise_dim = 32\n",
        "  dim = 128\n",
        "  batch_size = 256 if 256 <= train_samples.shape[0] else train_samples.shape[0]\n",
        "  \n",
        "  #train config\n",
        "  log_step = 20\n",
        "  epochs = 5+1\n",
        "  learning_rate = 5e-4\n",
        "  beta_1 = 0.5\n",
        "  beta_2 = 0.9\n",
        "\n",
        "  gan_args = ModelParameters(batch_size=batch_size,\n",
        "                           lr=learning_rate,\n",
        "                           betas=(beta_1, beta_2),\n",
        "                           noise_dim=noise_dim,\n",
        "                           n_cols=train_samples.shape[1],\n",
        "                           layers_dim=dim)\n",
        "  train_args = TrainParameters(epochs=epochs,\n",
        "                             sample_interval=log_step)\n",
        "  # Train GAN\n",
        "  model = WGAN_GP\n",
        "  synthesizer = model(gan_args, n_critic=5)\n",
        "  synthesizer.train(train_samples, train_args)\n",
        "\n",
        "  # Generate records based on random noise\n",
        "  generator = synthesizer.generator\n",
        "  rand_noise = np.random.normal(size=(generator_sample_size, noise_dim))\n",
        "  generated_samples = generator.predict(rand_noise)\n",
        "  return generated_samples"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdCx24Yxx-qu"
      },
      "source": [
        "def apply_synthesized_data(train_df, target_column, indexes, options, data = False):\n",
        "  train_df_drop_target = train_df.drop(columns = [target_column])\n",
        "  train_samples = train_df[train_df.index.isin(indexes)]\n",
        "\n",
        "  train_samples_positive = train_samples[train_samples[target_column] == 1].drop(columns = [target_column])\n",
        "  train_samples_negative = train_samples[train_samples[target_column] == 0].drop(columns = [target_column])\n",
        "\n",
        "  synthesized_positive = synthesized_data(train_samples_positive, options.get('generator_sample_size' ,train_samples_positive.shape[0] * 2))\n",
        "  synthesized_negative = synthesized_data(train_samples_negative, options.get('generator_sample_size' ,train_samples_negative.shape[0] * 2))\n",
        "\n",
        "  synthesized_positive_df = pd.DataFrame(synthesized_positive, columns = train_df_drop_target.columns)\n",
        "  synthesized_negative_df = pd.DataFrame(synthesized_negative, columns = train_df_drop_target.columns)\n",
        "\n",
        "  synthesized_positive_df[target_column] = np.ones(synthesized_positive_df.shape[0])\n",
        "  synthesized_negative_df[target_column] = np.zeros(synthesized_negative_df.shape[0])\n",
        "\n",
        "  if data:\n",
        "    return pd.concat([train_samples, synthesized_positive_df, synthesized_negative_df])\n",
        "\n",
        "  df_combined = pd.concat([train_df, synthesized_positive_df, synthesized_negative_df])\n",
        "\n",
        "  clf = XGBClassifier()\n",
        "  clf.fit(df_combined.drop(columns = [target_column]), df_combined[target_column])\n",
        "\n",
        "  return clf"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qnoofGHaqsS"
      },
      "source": [
        "####Ad Hoc Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OdsFJlEbHlc"
      },
      "source": [
        "def ad_hoc_model(train_df, target_column, indexes, options):\n",
        "\n",
        "  train_samples = apply_synthesized_data(train_df, target_column, indexes, options, True)\n",
        "\n",
        "  clf = XGBClassifier()\n",
        "  clf.fit(train_samples.drop(columns = [target_column]), train_samples[target_column])\n",
        "\n",
        "  return clf"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline Class "
      ],
      "metadata": {
        "id": "_ES-jEEbC7jg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbPgt2OlaFfq"
      },
      "source": [
        "class CustomFreyaAI:\n",
        "  def __init__(self, df, target_column = 'y'):\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "      raise Exception(\"data frame object must be of type 'pandas.core.frame.DataFrame'\")\n",
        "    if not target_column in df:\n",
        "      raise Exception(f\"The specified target column '{target_column}', was not found in the data frame\")\n",
        "    self.df = df.copy()\n",
        "    self.target_column = target_column\n",
        "\n",
        "  def get_slices_report(self, options = {}):\n",
        "    X_encoded, Y, categorical_features = initalize_data_set(target_column = self.target_column, predicted_column = '',df = self.df, categorical_threshold = options.get('categorical_threshold', 0.001))\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, Y, test_size=0.2, random_state=2, stratify=Y)\n",
        "\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = X_train, X_test, y_train, y_test\n",
        "\n",
        "    clf = XGBClassifier()\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    feature_slices, largest_feature_error_rates  = apply_heuristics(X_test, y_pred == y_test, categorical_features, options)\n",
        "\n",
        "    self.feature_slices = feature_slices\n",
        "    return feature_slices, largest_feature_error_rates\n",
        "\n",
        "  def apply_solution(self, feature_name, solution_name, options = {}):\n",
        "    if not feature_name in self.feature_slices:\n",
        "        raise Exception(f\"The specified feature name '{feature_name}', was not found among the features\")\n",
        "    if not solution_name in [\"reweighting\", \"data_synthesizer\", \"Ad_Hoc\"]:\n",
        "        raise Exception(f\"The specified solution name '{solution_name}', was not found among the available solutions\")\n",
        "\n",
        "    report = {}\n",
        "\n",
        "    X_train_copy = self.X_train.copy()\n",
        "    X_train_copy[self.target_column] = self.y_train\n",
        "    X_train_copy.reset_index(inplace = True)\n",
        "    X_train_copy.drop(columns = ['index'], inplace = True)\n",
        "\n",
        "    train_indexes = get_all_indexes_from_all_slices(X_train_copy, {feature_name:[self.feature_slices[feature_name]]})\n",
        "    test_indexes = get_all_indexes_from_all_slices(self.X_test, {feature_name:[feature_slices[feature_name]]})\n",
        "\n",
        "    clf = XGBClassifier()\n",
        "    clf.fit(self.X_train, self.y_train)\n",
        "    pred = clf.predict(self.X_test[self.X_test.index.isin(test_indexes)])\n",
        "    before_slice_score = metrics.accuracy_score(self.y_test[self.y_test.index.isin(test_indexes)], pred)\n",
        "    report['before_slice_score'] = before_slice_score\n",
        "\n",
        "    pred = clf.predict(self.X_test)\n",
        "    before_overall_score = metrics.accuracy_score(self.y_test, pred)\n",
        "    report['before_overall_score'] = before_overall_score\n",
        "\n",
        "    if solution_name == 'reweighting':\n",
        "      clf = reweighting(X_train_copy, self.target_column, train_indexes, options.get('weight', 5))\n",
        "    elif solution_name == 'data_synthesizer':\n",
        "      clf = apply_synthesized_data(X_train_copy, self.target_column, train_indexes, options)\n",
        "    elif solution_name == 'Ad_Hoc':\n",
        "      clf = ad_hoc_model(X_train_copy, self.target_column, train_indexes, options)\n",
        "\n",
        "    pred = clf.predict(self.X_test[self.X_test.index.isin(test_indexes)])\n",
        "    after_slice_score = metrics.accuracy_score(self.y_test[self.y_test.index.isin(test_indexes)], pred)\n",
        "    report['after_slice_score'] = after_slice_score\n",
        "\n",
        "    after_overall_score = None\n",
        "    \n",
        "    if solution_name == 'Ad_Hoc':\n",
        "      after_overall_score = before_overall_score\n",
        "    else:\n",
        "      pred = clf.predict(self.X_test)\n",
        "      after_overall_score = metrics.accuracy_score(self.y_test, pred)\n",
        "\n",
        "    report['after_overall_score'] = after_overall_score\n",
        "\n",
        "    report['after_to_before_slice_performance_ratio'] = after_slice_score / before_slice_score\n",
        "    report['after_to_before_overall_performance_ratio'] = after_overall_score / before_overall_score\n",
        "\n",
        "    return clf, report\n",
        "    \n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7tOTjJUUAkz"
      },
      "source": [
        "### Demo "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####German Credit Risk Data-set"
      ],
      "metadata": {
        "id": "d-UeR7WuFjLS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG3tuR5CSknl"
      },
      "source": [
        "PATH = \"{PATH}\"\n",
        "columns = ['Account Balance', 'Duration of Credit (month)',\n",
        "       'Payment Status of Previous Credit', 'Purpose', 'Credit Amount',\n",
        "       'Value Savings/Stocks', 'Length of current employment',\n",
        "       'Instalment per cent', 'Sex & Marital Status', 'Guarantors',\n",
        "       'Duration in Current address', 'Most valuable available asset',\n",
        "       'Age (years)', 'Concurrent Credits', 'Type of apartment',\n",
        "       'No of Credits at this Bank', 'Occupation', 'No of dependents',\n",
        "       'Telephone', 'Foreign Worker', 'Creditability']\n",
        "df = pd.read_csv(PATH + 'german.data', header = None, delimiter= ' ', names = columns)\n",
        "target = 'Creditability'\n",
        "df[target].replace((2), (0), inplace=True)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjZJojXldsxM"
      },
      "source": [
        "cf = CustomFreyaAI(df, target)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NfpCMHwX5zF"
      },
      "source": [
        "feature_slices, largest_feature_error_rates = cf.get_slices_report({'categorical_threshold': 0.001, 'eps': 0.05, 'hdr_threshold': 0.001})"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "field = \"Sex & Marital Status_Value Savings/Stocks\""
      ],
      "metadata": {
        "id": "bdAz0MoCVMGz"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__SwLJfwjUCI",
        "outputId": "1d11ad8d-b30f-40bb-b15e-7925736a9ba4"
      },
      "source": [
        "clf, report = cf.apply_solution(field, \"reweighting\", {'weight':50})\n",
        "report"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'after_overall_score': 0.69,\n",
              " 'after_slice_score': 0.5,\n",
              " 'after_to_before_overall_performance_ratio': 0.9718309859154929,\n",
              " 'after_to_before_slice_performance_ratio': 1.5,\n",
              " 'before_overall_score': 0.71,\n",
              " 'before_slice_score': 0.3333333333333333}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf, report = cf.apply_solution(field, \"data_synthesizer\", {'generator_sample_size': 500})\n",
        "report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lkvw4s78VVfM",
        "outputId": "01872ffd-f77f-410d-98f8-f45de44fbe3b"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 2/6 [00:05<00:08,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | disc_loss: -1428.65966796875 | gen_loss: 0.21422988176345825\n",
            "Epoch: 1 | disc_loss: -4095.9697265625 | gen_loss: -0.047602329403162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 4/6 [00:05<00:01,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | disc_loss: -8701.75390625 | gen_loss: -3.314283609390259\n",
            "Epoch: 3 | disc_loss: -14647.568359375 | gen_loss: -11.792234420776367\n",
            "Epoch: 4 | disc_loss: -19999.69140625 | gen_loss: -29.263248443603516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:05<00:00,  1.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 | disc_loss: -22676.404296875 | gen_loss: -61.65106964111328\n",
            "WARNING:tensorflow:6 out of the last 20 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb78e9d53b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 4/6 [00:05<00:01,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | disc_loss: -2414.059814453125 | gen_loss: -0.0750674232840538\n",
            "Epoch: 1 | disc_loss: -7389.974609375 | gen_loss: -0.27321910858154297\n",
            "Epoch: 2 | disc_loss: -17127.05859375 | gen_loss: -1.9202475547790527\n",
            "Epoch: 3 | disc_loss: -33216.64453125 | gen_loss: -10.759723663330078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 6/6 [00:05<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 | disc_loss: -56368.76171875 | gen_loss: -22.741825103759766\n",
            "Epoch: 5 | disc_loss: -86963.71875 | gen_loss: -34.859527587890625\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'after_overall_score': 0.695,\n",
              " 'after_slice_score': 0.3333333333333333,\n",
              " 'after_to_before_overall_performance_ratio': 0.9788732394366197,\n",
              " 'after_to_before_slice_performance_ratio': 1.0,\n",
              " 'before_overall_score': 0.71,\n",
              " 'before_slice_score': 0.3333333333333333}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf, report = cf.apply_solution(field, \"Ad_Hoc\", {'generator_sample_size': 500})\n",
        "report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7GAIgw7VWTM",
        "outputId": "dab45b6a-0d4b-41cc-a81f-f73797ba2ed4"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 3/6 [00:05<00:04,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | disc_loss: -1757.570068359375 | gen_loss: 0.031981322914361954\n",
            "Epoch: 1 | disc_loss: -4800.7314453125 | gen_loss: -1.0744318962097168\n",
            "Epoch: 2 | disc_loss: -9859.59375 | gen_loss: -5.260913372039795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:05<00:00,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | disc_loss: -15936.8515625 | gen_loss: -17.3005428314209\n",
            "Epoch: 4 | disc_loss: -20901.7265625 | gen_loss: -46.81269454956055\n",
            "Epoch: 5 | disc_loss: -22853.3359375 | gen_loss: -80.16205596923828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 17%|█▋        | 1/6 [00:05<00:27,  5.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | disc_loss: -1714.7637939453125 | gen_loss: 0.15792030096054077\n",
            "Epoch: 1 | disc_loss: -5164.39501953125 | gen_loss: -0.00025924481451511383\n",
            "Epoch: 2 | disc_loss: -11539.7314453125 | gen_loss: -0.008925974369049072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:05<00:00,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | disc_loss: -21830.009765625 | gen_loss: -2.721468210220337\n",
            "Epoch: 4 | disc_loss: -36664.66015625 | gen_loss: -11.077077865600586\n",
            "Epoch: 5 | disc_loss: -56529.50390625 | gen_loss: -35.97401809692383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'after_overall_score': 0.71,\n",
              " 'after_slice_score': 0.5,\n",
              " 'after_to_before_overall_performance_ratio': 1.0,\n",
              " 'after_to_before_slice_performance_ratio': 1.5,\n",
              " 'before_overall_score': 0.71,\n",
              " 'before_slice_score': 0.3333333333333333}"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Bank Marketing Campaign Data-set"
      ],
      "metadata": {
        "id": "8rgExFRRFtZ1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxHGD4GIX2Fe"
      },
      "source": [
        "PATH = \"{PATH}\"\n",
        "df = pd.read_csv(PATH + 'bank-additional-full.csv' ,delimiter=';')\n",
        "target = 'y'\n",
        "df[target].replace(('yes', 'no'), (1, 0), inplace=True)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cf = CustomFreyaAI(df, target)"
      ],
      "metadata": {
        "id": "ttfomddJCexB"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_slices, largest_feature_error_rates = cf.get_slices_report({'categorical_threshold': 0.001, 'eps': 0.05, 'hdr_threshold': 0.001})"
      ],
      "metadata": {
        "id": "aeCVVKBmCg4b"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "field = \"emp.var.rate_housing\""
      ],
      "metadata": {
        "id": "ZCd7Cia_WFIj"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf, report = cf.apply_solution(field, \"reweighting\", {'weight':50})\n",
        "report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyglQJcoCioU",
        "outputId": "bbbf1884-9877-4ac8-d64e-f78f4bf50acc"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'after_overall_score': 0.9118718135469774,\n",
              " 'after_slice_score': 0.6929824561403509,\n",
              " 'after_to_before_overall_performance_ratio': 0.9929940515532056,\n",
              " 'after_to_before_slice_performance_ratio': 1.025974025974026,\n",
              " 'before_overall_score': 0.9183054139354212,\n",
              " 'before_slice_score': 0.6754385964912281}"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf, report = cf.apply_solution(field, \"data_synthesizer\", {'generator_sample_size': 500})\n",
        "report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wbk3euOWCjo9",
        "outputId": "97744b1e-c598-4836-df84-efd5e25f3f7e"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:03<00:15,  3.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | disc_loss: 3.400144577026367 | gen_loss: 0.09514334052801132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 2/6 [00:03<00:05,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | disc_loss: -9.223896026611328 | gen_loss: 0.026753904297947884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 3/6 [00:03<00:02,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | disc_loss: -17.060903549194336 | gen_loss: -0.010062932036817074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 4/6 [00:04<00:01,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | disc_loss: -16.846355438232422 | gen_loss: 0.08231322467327118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 5/6 [00:04<00:00,  1.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 | disc_loss: -27.385746002197266 | gen_loss: 0.06361714750528336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:04<00:00,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 | disc_loss: -17.930011749267578 | gen_loss: -0.08807382732629776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 17%|█▋        | 1/6 [00:03<00:16,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | disc_loss: 10.523080825805664 | gen_loss: 0.029022052884101868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 2/6 [00:03<00:06,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | disc_loss: 0.5212745666503906 | gen_loss: 0.012305536307394505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 3/6 [00:03<00:03,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | disc_loss: -2.7973155975341797 | gen_loss: -0.017641814425587654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 4/6 [00:04<00:01,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | disc_loss: -5.115392684936523 | gen_loss: -0.032688841223716736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 5/6 [00:04<00:00,  1.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 | disc_loss: -8.934596061706543 | gen_loss: -0.04363608360290527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:05<00:00,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 | disc_loss: -13.629565238952637 | gen_loss: -0.14978353679180145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'after_overall_score': 0.9197620781743141,\n",
              " 'after_slice_score': 0.7017543859649122,\n",
              " 'after_to_before_overall_performance_ratio': 1.0015862524785195,\n",
              " 'after_to_before_slice_performance_ratio': 1.0389610389610389,\n",
              " 'before_overall_score': 0.9183054139354212,\n",
              " 'before_slice_score': 0.6754385964912281}"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf, report = cf.apply_solution(field, \"Ad_Hoc\", {'generator_sample_size': 200})\n",
        "report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QXj3u1MV-wZ",
        "outputId": "998170a8-9772-4b65-dc13-ce45bd2d12cd"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 1/6 [00:03<00:15,  3.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | disc_loss: -2.6936187744140625 | gen_loss: 0.11010019481182098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 2/6 [00:03<00:05,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | disc_loss: -19.116252899169922 | gen_loss: -0.0023951518815010786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 3/6 [00:03<00:02,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | disc_loss: -16.98705291748047 | gen_loss: -0.04965997859835625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 4/6 [00:03<00:01,  1.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | disc_loss: -27.71735382080078 | gen_loss: -0.10684904456138611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 5/6 [00:04<00:00,  1.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 | disc_loss: -28.418479919433594 | gen_loss: -0.18876804411411285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:04<00:00,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 | disc_loss: -32.51194381713867 | gen_loss: -0.2880309522151947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 17%|█▋        | 1/6 [00:03<00:16,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | disc_loss: 14.807206153869629 | gen_loss: -0.022317200899124146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 2/6 [00:03<00:06,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | disc_loss: -2.113308906555176 | gen_loss: -0.03788308426737785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 3/6 [00:03<00:03,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | disc_loss: -0.7218551635742188 | gen_loss: -0.04836206138134003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 4/6 [00:04<00:01,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | disc_loss: -12.450512886047363 | gen_loss: -0.10760579258203506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 5/6 [00:04<00:00,  1.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 | disc_loss: -11.416218757629395 | gen_loss: -0.13973498344421387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:05<00:00,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 | disc_loss: -14.059602737426758 | gen_loss: -0.2286074161529541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'after_overall_score': 0.9183054139354212,\n",
              " 'after_slice_score': 0.6929824561403509,\n",
              " 'after_to_before_overall_performance_ratio': 1.0,\n",
              " 'after_to_before_slice_performance_ratio': 1.025974025974026,\n",
              " 'before_overall_score': 0.9183054139354212,\n",
              " 'before_slice_score': 0.6754385964912281}"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    }
  ]
}